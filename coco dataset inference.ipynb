{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65221c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a843b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf391225",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"/home/vivek.trivedi/ET623_project/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74623cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping param as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall param -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5807fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=\"/scratch/vivek.trivedi/VL_adapter/datasets/COCO/dataset_coco.json\"\n",
    "# with open(path, 'r') as json_file:\n",
    "#     karpathy_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9df665be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_images = 0\n",
    "# data = []\n",
    "# for datum in karpathy_data['images']:\n",
    "#         for d in datum['sentences']:\n",
    "#             #img_id = str(int(datum['filename'].split('.')[0].split('_')[-1]))\n",
    "# #             else:\n",
    "#             img_id = datum['filename'].split('.')[0]\n",
    "#             new_datum = {\n",
    "#                 'img_id': img_id,\n",
    "#                 'sent': d['raw'].strip(),\n",
    "#                 'targets': [d['raw'].strip() for d in datum['sentences']],\n",
    "#                 'is_train': False,\n",
    "#             }\n",
    "#             data.append(new_datum)\n",
    "#         n_images += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1112df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vivek.trivedi\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8acfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/vivek.trivedi/VL_adapter/VL-T5/src\n"
     ]
    }
   ],
   "source": [
    "cd \"/scratch/vivek.trivedi/VL_adapter/VL-T5/src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "335bb9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from param import parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b2f3394",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args(\n",
    "    parse=False,\n",
    "    backbone='facebook/bart-base',\n",
    "    load=\"/scratch/vivek.trivedi/VL_adapter/VL-T5/snap/VLBart_multitask/4tasks_hard_RN101_LMfull_bs100_image224_lr1e-4/LAST\"\n",
    ")\n",
    "args.gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "876eda06",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_dic={\"Full finetune\":\"/scratch/vivek.trivedi/VL_adapter/VL-T5/snap/VLBart_multitask/4tasks_hard_RN101_LMfull_bs100_image224_lr1e-4/LAST\",\n",
    "             \"Single Adapter\":\"/scratch/vivek.trivedi/VL_adapter/VL-T5/snap/VLBart_multitask/4tasks_hard_RN101_LMOneadapter+r8+ln_bs100_image224_lr1e-3/LAST\",\n",
    "             \"Multi Adapter\":\"/scratch/vivek.trivedi/VL_adapter/VL-T5/snap/VLBart_multitask/4tasks_hard_RN101_LMadapter+r8+ln_bs80_image224_lr3e-4/LAST\",\n",
    "             \"Hyperformer\":\"/scratch/vivek.trivedi/VL_adapter/VL-T5/snap/VLBart_multitask/4tasks_hard_RN101_LMhyperformer8+r8+ln_bs100_image224_lr1e-3/LAST\",\n",
    "             \"Single Compacter\":\"/scratch/vivek.trivedi/VL_adapter/VL-T5/snap/VLBart_multitask/4tasks_hard_RN101_LMOnecompacter+hdiv2+noshare+nofac+ln+prompt_bs100_image224_lr1e-3/LAST\",\n",
    "             \"Multi Compacter\":\"/scratch/vivek.trivedi/VL_adapter/VL-T5/snap/VLBart_multitask/4tasks_hard_RN101_LMcompacter+hdiv2+noshare+nofac+ln+prompt_bs100_image224_lr1e-3/LAST\",\n",
    "             \"Single LoRa\":\"/scratch/vivek.trivedi/VL_adapter/VL-T5/snap/VLBart_multitask/RN101_LMsinglelora128+lr1e-3_bs100_image224/LAST\",\n",
    "             \"Multi Lora\":\"/scratch/vivek.trivedi/VL_adapter/VL-T5/snap/VLBart_multitask/RN101_LMmultilora128+lr1e-3_bs100_image224/LAST\",\n",
    "             \"Prompt Tunning\":\"/scratch/vivek.trivedi/VL_adapter/VL-T5/snap/VLBart_multitask/4tasks_hard_RN101_LMprompt40_bs70_image224_lr1e-3/LAST\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87f924c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multitask import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efc9b955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model at GPU 1\n",
      "**************************************************\n",
      "Configurations\n",
      "{'RefCOCO_BUTD': False,\n",
      " 'RefCOCO_GT': False,\n",
      " 'adam_beta1': 0.9,\n",
      " 'adam_beta2': 0.999,\n",
      " 'adam_eps': 1e-06,\n",
      " 'add_adapter_cross_attn': True,\n",
      " 'add_layer_norm_after_adapter': False,\n",
      " 'add_layer_norm_before_adapter': False,\n",
      " 'additional_visual_embedding_layers': 0,\n",
      " 'answer_normalize': False,\n",
      " 'backbone': 'facebook/bart-base',\n",
      " 'batch_size': 256,\n",
      " 'caption_cocoonly': True,\n",
      " 'caption_only': False,\n",
      " 'classifier': False,\n",
      " 'clip_grad_norm': -1.0,\n",
      " 'cls_task': 'tinyimagenet',\n",
      " 'coco_only': False,\n",
      " 'comment': '',\n",
      " 'decoder_prompt_len': 0,\n",
      " 'deepspeed': None,\n",
      " 'distributed': False,\n",
      " 'do_lower_case': False,\n",
      " 'downsample': False,\n",
      " 'dropout': 0.1,\n",
      " 'dry': False,\n",
      " 'efficient_unique_hyper_net': False,\n",
      " 'encoder_prompt_len': 0,\n",
      " 'epochs': 12,\n",
      " 'expand_vis_embedding': False,\n",
      " 'factorized_phm': True,\n",
      " 'feat_dim': 2048,\n",
      " 'feature_type': 'butd',\n",
      " 'fp16': False,\n",
      " 'freeze_bn_statistics': False,\n",
      " 'freeze_ln_statistics': False,\n",
      " 'from_scratch': False,\n",
      " 'gen_max_length': 20,\n",
      " 'gpu': 1,\n",
      " 'gradient_accumulation_steps': 1,\n",
      " 'ground_upsample': 1,\n",
      " 'ground_weight': 1,\n",
      " 'hypercomplex_division': 4,\n",
      " 'image_size': '(448,448)',\n",
      " 'individual_vis_layer_norm': True,\n",
      " 'itm_cocoonly': True,\n",
      " 'lambda_z': 0.001,\n",
      " 'load': '/scratch/vivek.trivedi/VL_adapter/VL-T5/snap/VLBart_multitask/4tasks_hard_RN101_LMfull_bs100_image224_lr1e-4/LAST',\n",
      " 'load_lxmert_qa': None,\n",
      " 'local_rank': -1,\n",
      " 'log_train_accuracy': False,\n",
      " 'lora_alpha': 32,\n",
      " 'lora_dim': 4,\n",
      " 'losses': 'lm,obj,attr,feat',\n",
      " 'low_rank_rank': 1,\n",
      " 'lr': 0.0001,\n",
      " 'max_n_boxes': 36,\n",
      " 'max_text_length': 20,\n",
      " 'mid_dim': 768,\n",
      " 'multiGPU': False,\n",
      " 'multitask_sampling': 'roundrobin',\n",
      " 'n_boxes': 36,\n",
      " 'n_ground': 1,\n",
      " 'n_image_tokens': 4,\n",
      " 'no_prefix': False,\n",
      " 'num_beams': 1,\n",
      " 'num_workers': 0,\n",
      " 'obj_mask_rate': 0.15,\n",
      " 'oneddownsample': False,\n",
      " 'optim': 'adamw',\n",
      " 'optimizer': 'adamw',\n",
      " 'oscar_tags': False,\n",
      " 'output': 'snap/test',\n",
      " 'phm_init_range': 0.01,\n",
      " 'phm_rank': 1,\n",
      " 'pos_dim': 4,\n",
      " 'post_prompt': '',\n",
      " 'prefix': None,\n",
      " 'projected_task_embedding_dim': -1,\n",
      " 'prompt': 'vqa: ',\n",
      " 'raw_label': False,\n",
      " 'reduction_factor': 16,\n",
      " 'remove_bn_vis_adapter': False,\n",
      " 'run_name': '',\n",
      " 'seed': 9595,\n",
      " 'share_down_sampler': False,\n",
      " 'share_up_sampler': False,\n",
      " 'share_vis_lang_layer_norm': False,\n",
      " 'shared_phm_rule': True,\n",
      " 'shared_phm_rule_over_tasks': False,\n",
      " 'shuffle_boxes': False,\n",
      " 'single_vqa_prefix': False,\n",
      " 'sparse_sample': False,\n",
      " 'submit': False,\n",
      " 'tasks': '',\n",
      " 'test': None,\n",
      " 'test_answerable': False,\n",
      " 'test_only': False,\n",
      " 'testing': False,\n",
      " 'tokenizer': 'facebook/bart-base',\n",
      " 'track_z': False,\n",
      " 'train': 'train',\n",
      " 'train_topk': -1,\n",
      " 'unfreeze_batch_norms': False,\n",
      " 'unfreeze_language_model': False,\n",
      " 'unfreeze_layer_norms': False,\n",
      " 'unfreeze_lm_head': False,\n",
      " 'unfreeze_vis_encoder': False,\n",
      " 'unfreeze_vis_last_layer': False,\n",
      " 'unique_hyper_net': False,\n",
      " 'use_adam_for_visual': False,\n",
      " 'use_adapter': False,\n",
      " 'use_attn_prefix': False,\n",
      " 'use_compacter': False,\n",
      " 'use_data_augmentation': False,\n",
      " 'use_hyperformer': False,\n",
      " 'use_lm_head_adapter': False,\n",
      " 'use_lora': False,\n",
      " 'use_lradapter': False,\n",
      " 'use_separate_optimizer_for_visual': False,\n",
      " 'use_single_adapter': False,\n",
      " 'use_single_lora': False,\n",
      " 'use_single_prompt': False,\n",
      " 'use_tasks_prompts': False,\n",
      " 'use_vis_adapter': False,\n",
      " 'use_vis_layer_norm': True,\n",
      " 'use_vis_order_embedding': True,\n",
      " 'use_vision': True,\n",
      " 'valid': 'valid',\n",
      " 'valid_batch_size': None,\n",
      " 'valid_topk': -1,\n",
      " 'vis_adapter_type': 'middle-bottleneck',\n",
      " 'vis_lr': 0.0001,\n",
      " 'vis_pointer': False,\n",
      " 'vis_pooling_output': False,\n",
      " 'vis_reduction_factor': 2,\n",
      " 'vis_use_transformer': False,\n",
      " 'vis_weight_decay': 0.01,\n",
      " 'warmup_ratio': 0.05,\n",
      " 'weight_decay': 0.01,\n",
      " 'word_mask_rate': 0.15}\n",
      "**************************************************\n",
      "Model loaded from  /scratch/vivek.trivedi/VL_adapter/VL-T5/snap/VLBart_multitask/4tasks_hard_RN101_LMfull_bs100_image224_lr1e-4/LAST.pth\n",
      "<All keys matched successfully>\n",
      "Model Launching at GPU 1\n",
      "model.encoder.visual_embedding.feat_embedding.0.weight is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.0.bias is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.1.weight is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.1.bias is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.0.weight is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.0.bias is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.1.weight is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.1.bias is trainable...\n",
      "model.encoder.visual_embedding.img_order_embedding.weight is trainable...\n",
      "Trainable param percentage: 1.12% (1582848/141156864)\n",
      "It took 7.9s\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(args,\n",
    "                  train=False\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b215d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast, BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\n",
    "    args.backbone,\n",
    "    # max_length=self.args.max_text_length,\n",
    "    do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79dfb8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_build(index):\n",
    "    image_name=data[index]['img_id']\n",
    "    path=\"/scratch/vivek.trivedi/VL_adapter/datasets/COCO/clip_features/data_clip_RN101_att/\"+image_name+\".h5\"\n",
    "    with h5py.File(path, 'r') as file:\n",
    "        features = file[image_name][\"features\"][:]\n",
    "        features=torch.tensor(features)\n",
    "    input_text = ''\n",
    "    input_ids = []\n",
    "    input_ids =tokenizer.encode(input_text)\n",
    "    B=1\n",
    "    input_ids=torch.LongTensor([input_ids]).type(torch.long)\n",
    "    boxes = torch.zeros(B,features.shape[0], 4,dtype=torch.float)\n",
    "    features=torch.unsqueeze(features, dim=0).type(torch.float)\n",
    "    batch={}\n",
    "    batch['vis_feats'] = features # (L, D)\n",
    "    batch['boxes'] = boxes\n",
    "    batch['task']=\"caption\"\n",
    "    batch[\"input_ids\"]=input_ids\n",
    "    result = trainer.model.test_step(batch)\n",
    "    print(\"prediction of model is \")\n",
    "    print(result['pred'])\n",
    "    if 'val2014' in image_name:\n",
    "        image_path=\"/scratch/vivek.trivedi/VL_adapter/datasetsp/COCO/images/val2014/\"+image_name+\".jpg\"\n",
    "    elif 'train2014' in image_name:\n",
    "        image_path=\"/scratch/vivek.trivedi/VL_adapter/datasetsp/COCO/images/train2014/\"+image_name+\".jpg\"\n",
    "    elif 'test2015' in image_name:\n",
    "        image_path=\"/scratch/vivek.trivedi/VL_adapter/datasetsp/COCO/images/test2015/\"+image_name+\".jpg\"\n",
    "    else:\n",
    "        None\n",
    "    img = mpimg.imread(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b37e9502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption_build(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25e622ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clip_visual_feats(image_name):\n",
    "    os.chdir('/home/vivek.trivedi/OpenAI-CLIP-Feature/')\n",
    "    command = f\"\"\"\\\n",
    "    CUDA_VISIBLE_DEVICES=1 python3 clip_visual_feats.py \\\n",
    "    --image_list {image_name} \\\n",
    "    --image_dir \"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" \\\n",
    "    --output_dir \"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\" \\\n",
    "    --ve_name 'RN101' \\\n",
    "    --model_type_or_path 'RN101'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run the command within the current Python environment\n",
    "        subprocess.run(command, shell=True, check=True, executable=\"/bin/bash\")\n",
    "        print(\"Command executed successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error: \", e)\n",
    "    os.chdir('/home/vivek.trivedi/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e217e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vivek.trivedi\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f3cb3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/vivek.trivedi/VL_adapter/VL-T5/src\n"
     ]
    }
   ],
   "source": [
    "cd \"/scratch/vivek.trivedi/VL_adapter/VL-T5/src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "610be5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_dict={}\n",
    "for key,value in trainer_dic.items():\n",
    "    args = parse_args(\n",
    "        parse=False,\n",
    "        backbone='facebook/bart-base',\n",
    "        load=value\n",
    "    )\n",
    "    args.gpu = 1\n",
    "    trainer_ = None\n",
    "    trainer_dict[key]=trainer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40aa090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_build(image_name,task,question):\n",
    "    run_clip_visual_feats(image_name)\n",
    "    image_path=\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\"+image_name\n",
    "    features=torch.tensor(np.load(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\"+image_name+\".npz\")['features'])\n",
    "    input_text = question\n",
    "    input_ids = []\n",
    "    input_ids =tokenizer.encode(input_text)\n",
    "    B=1\n",
    "    input_ids=torch.LongTensor([input_ids]).type(torch.long)\n",
    "    boxes = torch.zeros(B,features.shape[0], 4,dtype=torch.float)\n",
    "    features=torch.unsqueeze(features, dim=0).type(torch.float)\n",
    "    batch={}\n",
    "    batch['vis_feats'] = features # (L, D)\n",
    "    batch['boxes'] = boxes\n",
    "    batch['task']=task\n",
    "    batch[\"input_ids\"]=input_ids\n",
    "    result = trainer.model.test_step(batch)\n",
    "    print(\"prediction of model is \")\n",
    "    print(result)\n",
    "    img = mpimg.imread(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6d0e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_pred(image,question,trainer_adapter):\n",
    "    list_vq=list(trainer_dic.keys())\n",
    "    if trainer_adapter!=None:\n",
    "        trainer_ = trainer_dict[list_vq[trainer_adapter]]\n",
    "    image_name=\"image1.jpg\"\n",
    "    image.save(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name)\n",
    "    run_clip_visual_feats(image_name)\n",
    "    image_path=\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\"+image_name\n",
    "    features=torch.tensor(np.load(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\"+image_name+\".npz\")['features'])\n",
    "    input_text = question\n",
    "    input_ids = []\n",
    "    input_ids =tokenizer.encode(input_text)\n",
    "    B=1\n",
    "    input_ids=torch.LongTensor([input_ids]).type(torch.long)\n",
    "    boxes = torch.zeros(B,features.shape[0], 4,dtype=torch.float)\n",
    "    features=torch.unsqueeze(features, dim=0).type(torch.float)\n",
    "    batch={}\n",
    "    batch['vis_feats'] = features # (L, D)\n",
    "    batch['boxes'] = boxes\n",
    "    batch['task']='vqa'\n",
    "    batch[\"input_ids\"]=input_ids\n",
    "    result = trainer.model.test_step(batch)\n",
    "    if os.path.exists(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name):\n",
    "        os.remove(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name)\n",
    "    if os.path.exists(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\" + image_name + \".npz\"):\n",
    "        os.remove(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\" + image_name + \".npz\")\n",
    "    return result[\"pred_ans\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c8a74d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_pred(image,trainer_adapter):\n",
    "    list_vq=list(trainer_dic.keys())\n",
    "    if trainer_adapter!=None:\n",
    "        trainer_ = trainer_dict[list_vq[trainer_adapter]]\n",
    "    question=\"\"\n",
    "    image_name=\"image1.jpg\"\n",
    "    image.save(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name)\n",
    "    run_clip_visual_feats(image_name)\n",
    "    image_path=\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\"+image_name\n",
    "    features=torch.tensor(np.load(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\"+image_name+\".npz\")['features'])\n",
    "    input_text = question\n",
    "    input_ids = []\n",
    "    input_ids =tokenizer.encode(input_text)\n",
    "    B=1\n",
    "    input_ids=torch.LongTensor([input_ids]).type(torch.long)\n",
    "    boxes = torch.zeros(B,features.shape[0], 4,dtype=torch.float)\n",
    "    features=torch.unsqueeze(features, dim=0).type(torch.float)\n",
    "    batch={}\n",
    "    batch['vis_feats'] = features # (L, D)\n",
    "    batch['boxes'] = boxes\n",
    "    batch['task']='caption'\n",
    "    batch[\"input_ids\"]=input_ids\n",
    "    result = trainer.model.test_step(batch)\n",
    "    if os.path.exists(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name):\n",
    "        os.remove(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name)\n",
    "    if os.path.exists(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\" + image_name + \".npz\"):\n",
    "        os.remove(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\" + image_name + \".npz\")\n",
    "    return result[\"pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29262209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gqa_pred(image,question,trainer_adapter):\n",
    "    list_vq=list(trainer_dic.keys())\n",
    "    if trainer_adapter!=None:\n",
    "        trainer_ = trainer_dict[list_vq[trainer_adapter]]\n",
    "    image_name=\"image1.jpg\"\n",
    "    image.save(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name)\n",
    "    run_clip_visual_feats(image_name)\n",
    "    image_path=\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\"+image_name\n",
    "    features=torch.tensor(np.load(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\"+image_name+\".npz\")['features'])\n",
    "    input_text = question\n",
    "    input_ids = []\n",
    "    input_ids =tokenizer.encode(input_text)\n",
    "    B=1\n",
    "    input_ids=torch.LongTensor([input_ids]).type(torch.long)\n",
    "    boxes = torch.zeros(B,features.shape[0], 4,dtype=torch.float)\n",
    "    features=torch.unsqueeze(features, dim=0).type(torch.float)\n",
    "    batch={}\n",
    "    batch['vis_feats'] = features # (L, D)\n",
    "    batch['boxes'] = boxes\n",
    "    batch['task']='gqa'\n",
    "    batch[\"input_ids\"]=input_ids\n",
    "    result = trainer.model.test_step(batch)\n",
    "    if os.path.exists(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name):\n",
    "        os.remove(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name)\n",
    "    if os.path.exists(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\" + image_name + \".npz\"):\n",
    "        os.remove(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\" + image_name + \".npz\")\n",
    "    return result[\"pred_ans\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f9041f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlvr_pred(image1, image2, question,trainer_adapter):\n",
    "    list_vq=list(trainer_dic.keys())\n",
    "    if trainer_adapter!=None:\n",
    "        trainer_ = trainer_dict[list_vq[trainer_adapter]]\n",
    "    image_name1=\"image1.jpg\"\n",
    "    image_name2=\"image2.jpg\"\n",
    "    image1.save(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name1)\n",
    "    image2.save(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name2)\n",
    "    run_clip_visual_feats(image_name1)\n",
    "    image_path1 = \"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name1\n",
    "    features1 = torch.tensor(np.load(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\" + image_name1 + \".npz\")['features'])\n",
    "\n",
    "    run_clip_visual_feats(image_name2)\n",
    "    image_path2 = \"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name2\n",
    "    features2 = torch.tensor(np.load(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\" + image_name2 + \".npz\")['features'])\n",
    "\n",
    "    input_text = question\n",
    "    input_ids = tokenizer.encode(input_text)\n",
    "    input_ids = torch.LongTensor([input_ids]).type(torch.long)\n",
    "\n",
    "    B = 1\n",
    "    features1 = torch.unsqueeze(features1, dim=0).unsqueeze(0).type(torch.float)\n",
    "    features2 = torch.unsqueeze(features2, dim=0).unsqueeze(0).type(torch.float)\n",
    "\n",
    "    # Concatenate features1 and features2 along a new dimension 1\n",
    "    features = torch.cat((features1, features2), dim=1)\n",
    "    boxes1 = torch.zeros(B, features1.shape[2], 4, dtype=torch.float).unsqueeze(0)\n",
    "    boxes2 = torch.zeros(B, features2.shape[2], 4, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "    # Concatenate boxes1 and boxes2 along a new dimension 1\n",
    "    boxes = torch.cat((boxes1, boxes2), dim=1)\n",
    "    batch = {}\n",
    "    batch['vis_feats'] = features  # (L, D)\n",
    "    batch['boxes'] = boxes\n",
    "    batch['task'] = 'nlvr'\n",
    "    batch[\"input_ids\"] = input_ids\n",
    "\n",
    "    result = trainer.model.test_step(batch)\n",
    "    if os.path.exists(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name1):\n",
    "        os.remove(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name1)\n",
    "    if os.path.exists(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\" + image_name1 + \".npz\"):\n",
    "        os.remove(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\" + image_name1 + \".npz\")\n",
    "    if os.path.exists(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name2):\n",
    "        os.remove(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + image_name2)\n",
    "    if os.path.exists(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\" + image_name2 + \".npz\"):\n",
    "        os.remove(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\" + image_name2 + \".npz\")\n",
    "    if result['pred_ans_id'].any():\n",
    "        result_string = \"True\"\n",
    "    else:\n",
    "        result_string = \"False\"\n",
    "    return result_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76c2ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme = gr.themes.Base(\n",
    "    primary_hue=\"gray\",\n",
    "    secondary_hue=\"orange\",\n",
    "    neutral_hue=\"neutral\",\n",
    "    font=[gr.themes.GoogleFont('Source Sans Pro'), 'ui-sans-serif', gr.themes.GoogleFont('system-ui'), 'sans-serif'],\n",
    ").set(\n",
    "    body_background_fill='*primary_200',\n",
    "    body_background_fill_dark='*primary_950',\n",
    "    body_text_color_dark='*primary_100',\n",
    "    body_text_weight='500',\n",
    "    embed_radius='*radius_xxl',\n",
    "    background_fill_primary='*primary_50',\n",
    "    button_shadow='*input_shadow',\n",
    "    background_fill_primary_dark='*secondary_800',\n",
    "    background_fill_secondary_dark='*primary_200',\n",
    "    border_color_accent='*secondary_800',\n",
    "    border_color_accent_dark='*secondary_950',\n",
    "    border_color_primary='*primary_950',\n",
    "    color_accent_soft_dark='*neutral_500',\n",
    "    link_text_color='*neutral_900',\n",
    "    link_text_color_dark='*primary_200',\n",
    "    code_background_fill='*secondary_200',\n",
    "    block_background_fill='*body_background_fill',\n",
    "    button_border_width='*block_label_border_width',\n",
    "    button_primary_text_color_dark='*primary_300',\n",
    "    button_secondary_text_color_dark='*table_odd_background_fill',\n",
    "    button_secondary_text_color_hover='*button_secondary_background_fill'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0aacc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo():\n",
    "    list_vq=list(trainer_dic.keys())\n",
    "    with gr.Blocks(theme=theme) as demo:\n",
    "        \n",
    "        gr.Markdown(\n",
    "            \"\"\"<center><h2>Parameter Efficient training for Multimodal Tasks</center></h2>\n",
    "            <h3>Full finetuning</h3>\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        with gr.Tab(\"NLVR\"):\n",
    "            with gr.Row():\n",
    "                vq_btn = gr.Radio(list_vq, \\\n",
    "                    label=\"Adapter models\", type=\"index\", info=\"Choose your Adapter model\")\n",
    "            with gr.Row():\n",
    "                im1_nlvr=gr.Image(type=\"pil\", label=\"Upload an first image\")\n",
    "                im2_nlvr=gr.Image(type=\"pil\", label=\"Upload an second image\")\n",
    "            with gr.Row():\n",
    "                ques_nlvr=gr.Textbox(label=\"write question related to image\")\n",
    "            with gr.Row():\n",
    "                ans_nlvr=gr.Textbox(label=\"Answer\")\n",
    "            nlvr_submit = gr.Button(\"Generate NLVR Result...\")\n",
    "        with gr.Tab(\"VQA\"):\n",
    "            with gr.Row():\n",
    "                vq_btn = gr.Radio(list_vq, \\\n",
    "                    label=\"Adapter models\", type=\"index\", info=\"Choose your Adapter model\")\n",
    "            with gr.Row():\n",
    "                im1_vqa=gr.Image(type=\"pil\", label=\"Upload an image\")\n",
    "            with gr.Row():\n",
    "                ques_vqa=gr.Textbox(label=\"write question related to image\")\n",
    "            with gr.Row():\n",
    "                ans_vqa=gr.Textbox(label=\"Answer\")\n",
    "            vqa_submit = gr.Button(\"Generate VQA Result...\")\n",
    "        with gr.Tab(\"GQA\"):\n",
    "            with gr.Row():\n",
    "                vq_btn = gr.Radio(list_vq, \\\n",
    "                    label=\"Adapter models\", type=\"index\", info=\"Choose your Adapter model\")\n",
    "            with gr.Row():\n",
    "                im1_gqa=gr.Image(type=\"pil\", label=\"Upload an image\")\n",
    "            with gr.Row():\n",
    "                ques_gqa=gr.Textbox(label=\"write question related to image\")\n",
    "            with gr.Row():\n",
    "                ans_gqa=gr.Textbox(label=\"Answer\")\n",
    "            gqa_submit = gr.Button(\"Generate GQA Result...\")\n",
    "        with gr.Tab(\"Caption\"):\n",
    "            with gr.Row():\n",
    "                vq_btn = gr.Radio(list_vq, \\\n",
    "                    label=\"Adapter models\", type=\"index\", info=\"Choose your Adapter model\")\n",
    "            with gr.Row():\n",
    "                im1_cap=gr.Image(type=\"pil\", label=\"Upload an image\")\n",
    "            with gr.Row():\n",
    "                ans_cap=gr.Textbox(label=\"Answer\")\n",
    "            cap_submit = gr.Button(\"Generate caption Result...\")\n",
    "        nlvr_submit.click(nlvr_pred,inputs=[im1_nlvr,im2_nlvr,ques_nlvr,vq_btn],outputs=ans_nlvr)\n",
    "        vqa_submit.click(vqa_pred,inputs=[im1_vqa,ques_vqa,vq_btn],outputs=ans_vqa)\n",
    "        gqa_submit.click(gqa_pred,inputs=[im1_gqa,ques_gqa,vq_btn],outputs=ans_gqa)\n",
    "        cap_submit.click(cap_pred,inputs=[im1_cap,vq_btn],outputs=ans_cap)\n",
    "    demo.queue().launch(server_name='0.0.0.0',server_port=7072,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb834d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7072\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7072/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd961660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path=os.listdir(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\")\n",
    "# nlvr_pred(image_path[5],image_path[4],'nlvr',\"there are many person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78696461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def caption_build_u(image_name,task,question):\n",
    "#     run_clip_visual_feats(image_name)\n",
    "#     image_path=\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\"+image_name\n",
    "#     features=torch.tensor(np.load(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/features/\"+image_name+\".npz\")['features'])\n",
    "#     input_text = question\n",
    "#     input_ids = []\n",
    "#     input_ids =tokenizer.encode(input_text)\n",
    "#     B=1\n",
    "#     input_ids=torch.LongTensor([input_ids]).type(torch.long)\n",
    "#     boxes = torch.zeros(B,features.shape[0], 4,dtype=torch.float)\n",
    "#     features=torch.unsqueeze(features, dim=0).type(torch.float)\n",
    "#     batch={}\n",
    "#     batch['vis_feats'] = features # (L, D)\n",
    "#     batch['boxes'] = boxes\n",
    "#     batch['task']=task\n",
    "#     batch[\"input_ids\"]=input_ids\n",
    "#     result = trainer.model.test_step(batch)\n",
    "#     print(\"prediction of model is \")\n",
    "#     print(result)\n",
    "#     img = mpimg.imread(image_path)\n",
    "# #     plt.imshow(img)\n",
    "# #     plt.axis('off')\n",
    "# #     plt.show()\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipywidgets import interact, widgets, Output\n",
    "# from IPython.display import display, clear_output\n",
    "# import io\n",
    "# from PIL import Image\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def captionbuild(image, img_name, datatype, question):\n",
    "#     # Read and process the uploaded image\n",
    "#     content = image[list(image.keys())[0]]['content']\n",
    "#     img = Image.open(io.BytesIO(content))\n",
    "#     cv2.imwrite(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + img_name + \".jpg\", np.array(img))\n",
    "#     caption_build_u(img_name+\".jpg\",datatype,question)\n",
    "#     plt.imshow(img)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(img_name)\n",
    "#     plt.show()        \n",
    "\n",
    "# # Create file upload widget for the input image and input fields for image name, datatype, and question\n",
    "# upload_image = widgets.FileUpload(description=\"Upload Image\")\n",
    "# input_img_name = widgets.Text(description=\"Image Name:\")\n",
    "# input_datatype = widgets.Text(description=\"Datatype:\")\n",
    "# input_question = widgets.Text(description=\"Question:\")\n",
    "\n",
    "# # Create a submit button\n",
    "# submit_button = widgets.Button(description=\"Submit\")\n",
    "\n",
    "# # Create an output widget for displaying text output\n",
    "# text_output = Output()\n",
    "\n",
    "# # Function to be executed when the submit button is clicked\n",
    "# def on_submit_button_clicked(b):\n",
    "#     with text_output:\n",
    "#         clear_output()  # Clear previous output\n",
    "#         captionbuild(upload_image.value, input_img_name.value, input_datatype.value, input_question.value)\n",
    "\n",
    "# # Attach the function to the button click event\n",
    "# submit_button.on_click(on_submit_button_clicked)\n",
    "\n",
    "# # Display widgets and button\n",
    "# display(upload_image, input_img_name, input_datatype, input_question, submit_button, text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d278e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba44dc10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from flask import Flask, render_template, request, redirect, url_for\n",
    "# import io\n",
    "# from PIL import Image\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "# app = Flask(__name__)\n",
    "# CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "# @app.route('/')\n",
    "# def index():\n",
    "#     return render_template('index.html')\n",
    "\n",
    "# @app.route('/process', methods=['POST'])\n",
    "# def process():\n",
    "#     image = request.files['upload_image']\n",
    "#     img_name = request.form['input_img_name']\n",
    "#     datatype = request.form['action']\n",
    "#     question = request.form.get('input_question', '')\n",
    "#     content = image.read()\n",
    "#     img = Image.open(io.BytesIO(content))\n",
    "#     max_size = (800, 600)\n",
    "#     img.thumbnail(max_size)\n",
    "#     # Save the resized image\n",
    "# #     if img_name+\".jpg\" in os.listdir(\"static/uploaded_image/\"):\n",
    "# #         os.remove(\"static/uploaded_image/\" + img_name + \".jpg\")\n",
    "# #     if img_name+\".jpg\" in os.listdir(\"/home/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\"):\n",
    "# #         os.remove(\"/home/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + img_name + \".jpg\")\n",
    "# #         os.remove(\"/home/vivek.trivedi/VL_adapter/vivek_exp/features/\"+img_name+\".npz\")\n",
    "#     img.save(\"static/uploaded_image/\" + img_name + \".jpg\")\n",
    "#     cv2.imwrite(\"/scratch/vivek.trivedi/VL_adapter/vivek_exp/uploaded_image/\" + img_name + \".jpg\", np.array(img))\n",
    "#     caption=caption_build_u(img_name+\".jpg\",datatype,question)\n",
    "\n",
    "#     return  render_template('index.html', img_name=img_name, caption=caption)\n",
    "\n",
    "# # Create an HTML template string\n",
    "# html_template = \"\"\"\n",
    "# <!DOCTYPE html>\n",
    "# <html>\n",
    "# <head>\n",
    "#     <title>Vision Question Answering Adapter</title>\n",
    "# </head>\n",
    "# <body>\n",
    "#     <h1>Image Captioning</h1>\n",
    "#     <form action=\"/process\" method=\"POST\" enctype=\"multipart/form-data\">\n",
    "#         <label for=\"upload_image\">Upload Image:</label>\n",
    "#         <input type=\"file\" name=\"upload_image\" required><br>\n",
    "        \n",
    "#         <label for=\"input_img_name\">Image Name:</label>\n",
    "#         <input type=\"text\" name=\"input_img_name\" required><br>\n",
    "        \n",
    "#         <label for=\"action\">Datatype:</label>\n",
    "#         <select name=\"action\" required>\n",
    "#             <option value=\"caption\">caption of image</option>\n",
    "#             <option value=\"vqa\">question answering 1</option>\n",
    "#         </select><br>\n",
    "        \n",
    "#         <label for=\"input_question\">Question:</label>\n",
    "#         <input type=\"text\" name=\"input_question\"><br>\n",
    "\n",
    "#         <input type=\"submit\" value=\"Submit\">\n",
    "#     </form>\n",
    "#     {% if img_name %}\n",
    "#     <h2>Uploaded Image: {{ img_name }}</h2>\n",
    "#     <img src=\"{{ url_for('static', filename='/uploaded_image/' + img_name +'.jpg') }}\" alt=\"Uploaded Image\">\n",
    "#     {% endif %}\n",
    "\n",
    "#     {% if caption %}\n",
    "#     <h2>Caption:</h2>\n",
    "#     <p>{{ caption }}</p>\n",
    "#     {% endif %}\n",
    "# </body>\n",
    "# </html>\n",
    "# \"\"\"\n",
    "\n",
    "# # Save the HTML template to a file\n",
    "# with open('templates/index.html', 'w') as f:\n",
    "#     f.write(html_template)\n",
    "\n",
    "# # Run the Flask app inside the Jupyter Notebook\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(host='0.0.0.0', port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f395de74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45894863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13898c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
